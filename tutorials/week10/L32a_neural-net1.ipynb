{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural networks: predict\n",
    "\n",
    "CPSC 340: Machine Learning and Data Mining\n",
    "\n",
    "The University of British Columbia\n",
    "\n",
    "2018 Winter Term 1\n",
    "\n",
    "Mike Gelbart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.random as npr\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "%autosave 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Back to supervised learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider 1-D regression..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([1,3,4,  5,5.2,6])\n",
    "y = np.array([1,2,2.3,3,3.5,4.8])    \n",
    "def plot_pts(x,y):\n",
    "    plt.plot(x,y, '.', markersize=15)\n",
    "    plt.xlim([0, 7])\n",
    "    plt.ylim([0, 5])\n",
    "plot_pts(x,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider various fits... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pts(x,y)\n",
    "grid = np.linspace(0,7,100)\n",
    "plt.plot(grid,0.7*grid)\n",
    "plt.title(\"Linear fit: $y=ax+b$ (2 parameters)\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pts(x,y)\n",
    "plt.plot(grid,0.4*grid+0.05*grid**2)\n",
    "plt.title(\"Quadratic fit: $y=ax^2+bx+c$ (3 parameters)\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can fit various functions from the _polynomial family_. But each polynomial **has a different number of parameters**. Note that the polynomial family always maps from $\\mathbb{R}^d\\rightarrow \\mathbb{R}$.\n",
    "\n",
    "- Model: has parameters that we fit\n",
    "- Family: a set of models that are linked by some theme, but may have different numbers of parameters\n",
    "- **\"Neural network\" is a family of functions just like this.**\n",
    "  - The class includes linear functions like we saw above, but also insanely nonlinear ones.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Formal definition\n",
    "\n",
    "$$ z^{(l+1)} = f\\left( W^{(l)} z^{(l)} + b^{(l)}\\right) $$\n",
    "\n",
    "where $W^{(l)}$ is a matrix of parameters, $b^{(l)}$ is a vector of parameters, $f(\\cdot)$ is an elementwise nonlinearity, $l=0,\\ldots,L$ where $L$ is the number of \"hidden layers\", $z^{(0)}=x$, and $z^{(L)}=y$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def example_1_layer_nn(x,W,f): # x is a vector, W is a matrix\n",
    "    return f(W@x)\n",
    "\n",
    "x = np.random.rand(5)\n",
    "W = np.random.rand(2,5)\n",
    "f = lambda x: 1/(1+np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_1_layer_nn(x,W,f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Above: the size of the input is 5 and the size of the output is 2.\n",
    "- Below: the size of the input is 5 and the size of the output is 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def example_nn(x,W,f): # x is a vector, W is a _list of matrices_\n",
    "    for W_l in W:\n",
    "        x = f(W_l@x)\n",
    "    return x\n",
    "\n",
    "x = np.random.rand(5)\n",
    "W1 = np.random.rand(2,5)\n",
    "W2 = np.random.rand(1,2)\n",
    "example_nn(x, [W1,W2], f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Regression: end with the prediction (output has $1$ dimension)\n",
    "- Classification: end with a softmax (output has $c$ dimensions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1D neural net example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate synthetic data\n",
    "npr.seed(5)\n",
    "n = 200\n",
    "X = npr.rand(n,1)\n",
    "y = np.sin(2*X[:,0]) + np.random.randn(n)*0.03\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(X,y,'.',markersize=10);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = MLPRegressor(solver='lbfgs')\n",
    "nn.fit(X,y);\n",
    "\n",
    "plt.plot(X,y,'.',markersize=10);\n",
    "grid = np.linspace(0,1,1000)[:,None]\n",
    "\n",
    "plt.plot(grid, nn.predict(grid),linewidth=5);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### random weights\n",
    "\n",
    "To build intuition about the types of functions we can represent, let's explore what happens with random weights using different architectures. Again, this is all within the confines of $\\mathbb{R}\\rightarrow\\mathbb{R}$ which isn't the real deal.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = MLPRegressor(hidden_layer_sizes=(), activation='logistic', max_iter=1)\n",
    "\n",
    "# change hidden_layer_sizes to, e.g. 10,\n",
    "# change activation to relu\n",
    "\n",
    "nn.fit(np.zeros((1,1)),[0])\n",
    "grid = np.linspace(-10,10,1000)[:,None]\n",
    "plt.plot(grid, nn.predict(grid),linewidth=2);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regression example with $d=2$ (again, change hidden layer sizes and nonlinearity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = MLPRegressor(hidden_layer_sizes=(), max_iter=1)\n",
    "\n",
    "nn.fit(np.zeros((1,2)),[0])\n",
    "fig = plt.figure()\n",
    "ax = plt.axes(projection='3d')\n",
    "\n",
    "ngrid = 100\n",
    "X = np.linspace(-5, 5, ngrid)\n",
    "Y = np.linspace(-5, 5, ngrid)\n",
    "X, Y = np.meshgrid(X, Y)\n",
    "\n",
    "inputs = np.append(X.flatten()[:,None], Y.flatten()[:,None],axis=1)\n",
    "outputs = nn.predict(inputs)\n",
    "Z = np.reshape(outputs, [ngrid,ngrid])\n",
    "\n",
    "# Plot the surface.\n",
    "surf = ax.plot_surface(X, Y, Z, cmap=cm.coolwarm, linewidth=0, antialiased=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some deep learning software\n",
    "\n",
    "There's been a lot of software released lately to take care of this for you. Some big players are:\n",
    "\n",
    "| Name   |  Host language  | Released |\n",
    "|--------|-------------|---------------|\n",
    "| [Torch](http://torch.ch) | Lua | 2002 |\n",
    "| [Theano](http://deeplearning.net/software/theano/) | Python | 2007 |\n",
    "| [Caffe](http://caffe.berkeleyvision.org) | Executable with Python wrapper | 2014 |\n",
    "| [TensorFlow](https://www.tensorflow.org) | Python | 2015 |\n",
    "| [Keras](https://keras.io) | Python | 2015 |\n",
    "| [PyTorch](http://pytorch.org) | Python | 2017 |\n",
    "| [Caffe 2](https://caffe2.ai/) | Python or C++ | 2017 |\n",
    "\n",
    "There are many others. See for example [Comparison of deep learning software](https://en.wikipedia.org/wiki/Comparison_of_deep_learning_software)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Repeating the examples above in Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate fake data\n",
    "npr.seed(5)\n",
    "N = 200\n",
    "X = npr.rand(N,1)\n",
    "y = np.sin(2*X) + npr.randn(N,1)*0.03\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(10, input_dim=1, activation='tanh', kernel_initializer='lecun_uniform',))\n",
    "model.add(Dense(1, activation='linear', kernel_initializer='lecun_uniform',))\n",
    "\n",
    "# Compile model\n",
    "sgd = keras.optimizers.SGD(learning_rate=0.1, momentum=0.0, decay=0.0, nesterov=False)\n",
    "model.compile(loss='mean_squared_error', optimizer=sgd)#'adam')\n",
    "\n",
    "# Fit the model\n",
    "loss=model.evaluate(X, y,verbose=0)\n",
    "print(loss)\n",
    "model.fit(X, y, epochs=1000, verbose=0)\n",
    "\n",
    "# evaluate the model\n",
    "loss=model.evaluate(X, y,verbose=0)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Note that no code is executed in the first few lines.\n",
    "- This is a new paradigm that we're not exactly used to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(X,y,'.',markersize=10)\n",
    "grid = np.linspace(0,1,1000)[:,None]\n",
    "plt.plot(grid, model.predict(grid),linewidth=5);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random weights\n",
    "model = Sequential()\n",
    "model.add(Dense(50, input_dim=2, activation='tanh', kernel_initializer='lecun_uniform'))\n",
    "model.add(Dense(1, activation='linear', kernel_initializer='lecun_uniform',))\n",
    "model.compile(loss='mean_squared_error', optimizer='sgd')\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = plt.axes(projection='3d')\n",
    "\n",
    "n = 100\n",
    "X = np.linspace(-5, 5, n)\n",
    "Y = np.linspace(-5, 5, n)\n",
    "X, Y = np.meshgrid(X, Y)\n",
    "\n",
    "inputs = np.append(X.flatten()[:,None], Y.flatten()[:,None],axis=1)\n",
    "outputs = model.predict(inputs)\n",
    "Z = np.reshape(outputs, [n,n])\n",
    "\n",
    "# Plot the surface.\n",
    "surf = ax.plot_surface(X, Y, Z, cmap=cm.coolwarm, linewidth=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How many parameters do we have?\n",
    "\n",
    "Just the weight matrices: $$dk_1 + k_1k_2 + \\ldots + k_{L-1}k_L + k_Lc$$\n",
    "\n",
    "With the biases: $$(d+1)k_1 + (k_1+1)k_2 + \\ldots + (k_{L-1}+1)k_L + (k_L+1)c$$\n",
    "\n",
    "where $c$ is the dimensionality of the output.\n",
    "  \n",
    "**This is potentially a lot of parameters!!!!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
